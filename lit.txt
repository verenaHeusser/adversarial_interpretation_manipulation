# Fooling Network Interpretation in Image Classification 
https://openaccess.thecvf.com/content_ICCV_2019/papers/Subramanya_Fooling_Network_Interpretation_in_Image_Classification_ICCV_2019_paper.pdf



CV, Grad CAM
- adversarial patches -> directly inspired by adversarial ml examples
- mostly, these pathces are highlighted by interpreters -> revealing the identity of the adversary


-> network prediction + interpreter are fooled

- adversary can change only pixels inside the patch
- patches are generated that change the image category into another target category -> incorporated into the loss
- loss used to suppress output of gradcam 

-> adversary has direct control over tha path



--------------------------------------------------
# Explanations can be manipulated 
and geometry is to blame
https://arxiv.org/pdf/1906.07983.pdf

CV, imagenet
- propose an algorithm which allows to manipulate an image with a hardly perceptibleperturbation such that the explanation matches an arbitrary target map
- demonstrate itseffectiveness for six different explanation methods and on four network architecture


- they also provide a theoretical analysis and give insight into the 

algorithm: optimization in order to generate an adversarial sample
- selection of 2 random images
- chose one to generate a target explanation map ht
- other image: perturbed with loss with the goal of replicating the target map ht
- gradient descent w.r.t. x_adv

Testing: 
- test the manipualted image with the original network

result: 
- image x and perturbed image are still quite similar
- original images explanation h_orig is very different to h_adv
- and h_adv closely respmbles the target h_t



show that dedicated imperceptible manipulations of the input data can yield arbitrary and drastic changes of the explanation map
-  arbitrary manipulation of model interpreter outcomes
- 



--------------------------------------------------

Fooling Neural Network Interpretations viaAdversarial Model Manipulation

- cv: adapted fine tuning stage of image classification models by using an altered fooling loss function
-> combination of classification loss (CE) and an additional adversarial term
-- part1: encourage the model to maintain prediction performance
-- part2: encourage the interpreter to give bad interpretations
=> model is robust, but interpreter is not



--------------------------------------------------
You Shouldnâ€™t Trust Me: Learning Models Which Conceal Unfairness FromMultiple Explanation Methods

http://ceur-ws.org/Vol-2560/paper8.pdf

n contrast, we penalise the gradient withrespect to specified target feature to reduce its importancescore.
We demonstrated that many popular explanation methods used in real-world settings are not able to indicate reliablywhether or not a model is fair. We provided an intuitive ex-planation to show how this can happen. We introduced amethod to modify an existing model and showed its empir-ical success in downgrading the feature importance of keysensitive features across six explanation methods and unseentest points across four datasets, while having little effect onmodel accuracy.



--------------------------------------------------
# ghorbani
https://arxiv.org/pdf/1710.10547.pdf
