1. fooling l+s

   1. propose a framework for fooling classifiers
   2. Our approach can be used to scaffoldany biased classifier in such a way that its predictions on the inputdata distribution still remain biased, but the post hoc explanationsof the scaffolded classifier look innocuous.
   3. --> fool the interpreter, as interpreter cannot detect biases of model
   4. thus, motivating the need fortools that can explain them in a faithful and interpretable manner.
   5. post-hoc, local explanation methods: Thesemethods estimate the contribution of individual features towards aspecific prediction by generating perturbations of a given instancein the data and observing the effect of these perturbations on theoutput of the black-box classifier.
   6. l+s learn an interpretable model around a black box classifier
   7. indroduce biases into the model by
   8. show that Lime and shap do not capture any of the important biases employed by the biased classifiers
   9. These results suggest that it is possible for malicious actorsto craft adversarial classifiers that are highly discriminatory, butcan effectively fool existing post hoc explanation techniques

2. https://arxiv.org/pdf/1710.10547.pdf ghorbani

   - interpreter fooling + explanation
   - We systematically characterize the robustness of interpreta-tions generated by several widely-used feature importanceinterpretation methods
   - In allcases, our experiments show that systematic perturbations canlead to dramatically different interpretations without chang-ing the label.
   - . Our analysis of the ge-ometry of the Hessian matrix gives insight on why robustnessis a general challenge to current interpretation approaches

   - n this pa-per, we introduce the notion of adversarial perturbations toneural network interpretation.
   - show that points near the decision line are especially susceptible to interpretability-based analysis
   - Our goal is to devise efficientand visually imperceptible perturbations that change the in-terpretability of the test input while preserving the predictedlabel.

3. the mythos

   - the term inerpretability has no agreed upon meaning
   - This paper makes a first step towards providing a compre-hensive taxonomy of both the desiderata and methods ininterpretability research
   - tries to leverage the problem of a lacking commonly accepted or even flawed problem definition
   - Even worse, we could imagine situations, like machinelearning for security, where the environment might be ac-tively adversarial
   - transparency, i.e.,how does the model work?Thesecond consists ofpost-hoc explanations, i.e.,what elsecan the model tell me? This division is a useful organi-zationally, but we note that it is not absolute. For examplepost-hoc analysis techniques attempt to uncover the signif-icance of various of parameters, an aim we group under theheading of transparency

4. saliency based text https://arxiv.org/pdf/1810.03292.pdf
   - Consequently, methods that fail the proposed tests are inadequate fortasks that are sensitive to either data or model, such as, finding outliers in the data,explaining the relationship between inputs and outputs that the model learned,and debugging the mode
   - model parameter randomization test
   - data randomization test

keywords:
black box explanations, model interpretability, bias detection, ad-versarial attacks

# interpretation methods

One way to understanding the behavior of such classifiers is to build simpler explanation models that are interpretable approximations of these black boxes.
The other way is to calculate feature importances for a given model.

(1) crafting of adversarial input examples
(2) crafting of adversarial classifiers / mdoels

It must be noted that interpretability is mostly assumed to be given if the following holds :

1. a model aiming to explain a more complex model achieves lower complexity and is thereby interpretable.
2. feature attribution is given, i.e. feature importances of single input features

An interpretation may prove informative even withoutshedding light on a modelâ€™s inner workings.

# questions:

- To quantify the perturba-tion size: [ghorbani et al.] -> l2, linf norms --> how much the images have been perturbed (in terms of number of pixels)?

# interpretation methods

- https://arxiv.org/pdf/1710.10547.pdf

1. post-hoc interpretations

   1. Feature importance interpretation

      - explains predictions in terms of realtive importance of features of an input test sample

      1. Simple Grad: 1st order lin approx of the model to detect the sensitivity of the score to perturbing each of the input dims
      2. Integrated Gradients:
      3. LRP methods: decompose the score h_i(x_i) backwards through the network
      4. DeepLift: Improved version of LRP

   2. Sample Importance Interpret
      - which training examples have the biggest effect on the model prediction
      1.

# comparision of interpretations -> metrics

(1) quantitatively

- https://arxiv.org/pdf/1710.10547.pdf
  --> comparsion of interpretations befoe and after perturbation.
  1.  Spearmans rank corr
  2.  top-k intersection: intersection of the k most important features
- FSR as a general measure of

(2) qualitatively

- by visual inspection

# IDAS

- mean resulting heatmap from location fooling?

# talks

## shafi goldberg @neurips

- in these times more important than ever to focus on what is true
- calfornia proposition 25: replace cash bail with risk assessment system --> automatic decision if people should stay in prison or not

  - ! only based on past data
  - not passed:
    - supporters: cash bail sys is racist, encourages the rich
    - opponents: civil rights advocates: while algs can pitch you a song or sell a toaster, they shouldn't be used for release decisions
      - the factors considered for release will still lead to people of color being held for trial at disproportionate rates.
        --> people don't trust the designers of the risk assessment sys

- general motivation:

  - risk assessment (PAC learning)
  - (formal) verification of ml algs: ideally, verifier shoudl have access to all data, mdoel, ...
  - currenly : verification of ml algs mostly wrt accuracy and efficiency
  - we want: robustness

- adversarial ml: where clever manipulations of an input by an adversary can cause misclassifications and fool applications
- mostly considering adversaries that input fake test samples

- paper:
  - ==> beyond paper: consider wc adversary without restricting strategies
  - robust learning algs for any concept class of bounded vc dimansion, for any adversarial test input distribution, when possible
  - mosly assumption for ml: train distribution p == test dist p
    - this is the problem with adv. examples: not same distr
    - paper: pq learning: when p and q deviate

# neurips papers

List of papers: https://neurips.cc/Conferences/2020/Schedule?type=Poster

- Ensuring Fairness Beyond the Training Data

- Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models

- How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods

  - investigation about which explanation humans prefer
  - https://github.com/nesl/ExMatchina
  - explainability study involving humans

- Certifiably Adversarially Robust Detection of Out-of-Distribution Data > outlook

- Adversarial robustness via robust low rank representations

# explanation why adv exapmeles work

- https://arxiv.org/pdf/1711.02846.pdf [2017]

# evaluations

- has already become a method to evaluate models wrt adversarial accuracy (depending on perturbation strength) [done in: https://arxiv.org/pdf/2012.04353.pdf]. so should interpreter robustness be tested, too

# comparison human vs non -human

- Humans are very robust against such perturbations; one possible reason could be that humans do not learn to classify based on an error between "target label" and "predicted label" but possibly due to reinforcements that they receive on their predictions.
- are humans also robust against perturbations of interpreters?
