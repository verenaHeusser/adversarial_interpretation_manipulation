%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2020}
% \acmYear{2020}
% \acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Explainability '20]{Interpretability '20: ACM Symposium on Neural Model Explanations}{December 16--20, 2020}{Karlsruhe, DE}
\acmBooktitle{Interpretability '20: ACM Symposium on Neural Model Explanations,
December 16--20, 2020, Karlsruhe, DE}
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}

% imports
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{titlesec}

% macros etc
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\mypar}[1]{\vspace{0.2cm}\noindent\textbf{#1}}
\setlength{\smallskipamount}{2mm}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\titleformat{\subsubsection}
  {\normalfont\normalsize\itshape}{\thesubsubsection}{1em}{}
\titlespacing*{\subsubsection}{0pt}{3.25ex plus 1ex minus .2ex}{0ex plus .2ex}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
% \title{Manipulating Model Explanations: How to fool what tries to make sense of}
\title{Why you shouldn't trust me: A survey on Adversarial Model .}
% \title{Manipulating Model Interpretations: Why you shouldn't trust me.}
% A survey on adversarial model interpreter manipulations

\author{Verena Heusser}
\email{verena.heusser@student.kit.edu}
\affiliation{%
  \institution{Karlsruhe Institute of Technology (KIT)}
  \city{Karlsruhe}
  \country{Germany}
}


\begin{abstract}
  The accuracy of machine learning models is no longer enough: we also want the often inherently complex models to be safe, robust, and interpretable by humans. As machine learning models enter critical areas in human lives such as the criminal justice system, medicine or financial systems, the inability for humans to understand these models is dangerous and problematic. Progress in the emerging research field of explainable artificial intelligence promises to be a remedy, where interpretation methods try to uncover \textit{how} models work. However, while the number of studies using these methods is exploding, there are also a growing number of articles pointing out that the interpretation methods are still fundamentally flawed.   
  This work provides an overview of the subfield of manipulating interpretable machine learning, with the goal of providing insight into concepts, existing research, and future directions. We want to raise awareness that interpretation methods, just like the underlying machine learning models, can be outwitted by adversaries and that there is often no way to detect adversarial attacks.
  
  % The main suggestion of this paper is to be cautious about results of explanation methods. Explanations can be fooled just as the underlying machine learning models. So. in the end the question must be posed whether inexplainable mdoels should be used at all if we need other models to explain these models but are not valid themselves .. % TODO loop structire aufbauen

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code
%%
\begin{CCSXML}
  % <ccs2012>
  % <concept>
  % <concept_id>10010147.10010257.10010293.10010294</concept_id>
  % <concept_desc>Computing methodologies~Neural networks</concept_desc>
  % <concept_significance>500</concept_significance>
  % </concept>
  % </ccs2012>
\end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately descr

%% the work being presented. Separate the keywords with commas.
\keywords{Interpretability, Adversarial Machine Learning, Adversarial Model Interpretation Manipulations}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

\maketitle


% manipulating explanations and explaining manipulations
\input{sections/introduction}

\input{sections/interpretation_methods}

\input{sections/manipulation_methods}

\input{sections/manipulations}

% \input{sections/explaining_manipulations}

% TODO add methods to reverse / disclose manipulations
% \input{sections/uncovering_manipulations}

\input{sections/benchmarking_interpretations}

% \input{sections/experiments}

\input{sections/discussion}

% TODO N neural network chengen to mathcal oder so
% TODO paper for iid assumption of nns?

% \balance
\bibliography{mybib}{}
% \bibliographystyle{plain}
\bibliographystyle{ACM-Reference-Format}


% \appendix

\end{document}
\endinput