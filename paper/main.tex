%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2020}
% \acmYear{2020}
% \acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Explainability '20]{Explainability '20: ACM Symposium on Neural Model Explanations}{December 16--20, 2020}{Karlsruhe, DE}
\acmBooktitle{Explainability '20: ACM Symposium on Neural Model Explanations,
December 16--20, 2020, Karlsruhe, DE}
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}

% imports
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{titlesec}

% macros etc
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\mypar}[1]{\vspace{0.3cm}\noindent\textbf{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}

\titleformat{\subsubsection}
  {\normalfont\normalsize\itshape}{\thesubsubsection}{1em}{}
\titlespacing*{\subsubsection}{0pt}{3.25ex plus 1ex minus .2ex}{0ex plus .2ex}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
% \title{Manipulating Model Explanations: How to fool what tries to make sense of}
\title{Manipulating Model Interpretations: Why you shouldn't trust me}
% A survey on adversarial model interpreter manipulations

\author{Verena Heusser}
\email{verena.heusser@student.kit.edu}
\affiliation{%
  \institution{Karlsruhe Institute of Technology (KIT)}
  \city{Karlsruhe}
  \country{Germany}
}


\begin{abstract}

  This paper provides a review on the subfield of manpipulating intprerpretable machine learning, with the aim to provide insight into concepts, existing research and future directions.  
  This paper reviews state-of-the-art approaches to model explanations with a focus on those techniques that try to 
  fool these methods. Based on the findings in the reviewed studies, we conclude that knowledge of the defectiveness of neural networks and their highly sought explanations should be taken with caution. Therefore, the main purpose of this review is to raise awareness that explanations, while helpful, can can be fundamentally flawed.

  We don't only want the model to be good. We want it to be safe and interpretable. 
  As machine learning models enter critical areas in human lives such as the criminal justice system, medicine or financial systems, the inability for humans to understand these models is dangerous and problematic. Advances in the rising research area of explainable AI seems to be a remedy. However, there is not yet a consensus about the validity and robustness of explanations methods themselves. The main suggestion of this paper is to be cautious about results of explanation methods. Explanations can be fooled just as the underlying machine learning models. So. in the end the question must be posed whether inexplainable mdoels should be used at all if we need other models to explain these models but are not valid themselves .. % TODO loop structire aufbauen

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code
%%
\begin{CCSXML}
  % <ccs2012>
  % <concept>
  % <concept_id>10010147.10010257.10010293.10010294</concept_id>
  % <concept_desc>Computing methodologies~Neural networks</concept_desc>
  % <concept_significance>500</concept_significance>
  % </concept>
  % </ccs2012>
\end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Interpretability, Neural networks, adversarial training}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

\maketitle


% manipulating explanations and explaining manipulations
\input{sections/introduction}

\input{sections/interpretation_methods}

\input{sections/manipulation_mothods}


\input{sections/manipulations}

\input{sections/explaining_manipulations}

% TODO add methods to reverse / disclose manipulations
\input{sections/uncovering_manipulations}

\input{sections/sanity_checks_for_interpreters}

\input{sections/benchmarking_interpretations}

% \input{sections/experiments}

\input{sections/discussion}

% TODO add code link
% TODO add author notes? -> replcation studies?
% TODO N neural network chengen to mathcal oder so
% TODO paper for iid assumption of nns?
% TODO x anpassen zu bf

% \balance
\bibliography{mybib}{}
% \bibliographystyle{plain}
\bibliographystyle{ACM-Reference-Format}


% \appendix

\end{document}
\endinput