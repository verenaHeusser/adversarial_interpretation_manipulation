\section{Discussion}

This paper summarizes the current approaches to manipulating model interpretation methods. 
On the one hand, the findings suggest that our models aro not fully aligned with how human information processing works. If machine learning models would decide by the criteria we humans employ for tasks such as image classification, there would be no fooling of interpretation models by input or model manipulations. 
On the other hand, it was shown that advances in machine learning models has led to models that rely too much on the data they are trained on, thus showing a high susceptibility to ood properties or properties that are highly correlated with labels in the dataset but are not distinctive in the real world (such as image backgrounds). Models and interpreters can still be misled in a large and systematic manner. 

However, we hope that the vastly expanding and progressing field of XAI will help to move towards more robust, reliable ond human-aligned machine learning models. 

While there exists a number of review papers on XAI and it's various subfields, this report is to the best of our knowledge the first one to comprehensively review manipulation methods for interpreters. 

We believe that identifying risks and adversaries helps to open up research on more robust interpretation methods. 

Interpretation methods can be categorized based on if they maintain local consistency among explanations (i.e. finding an explanation that is true for single data samples and their neighbors) or based on if they try to find global explanations, being true for all samples of a class. 
As there exist model manipulations methods, that structurally alter the models by adapting tre loss function, this line of global model fooling though being approached is still in its infancy. 


\subsection{Conclusion}
Finally, it must be noted that the suitability of a method depends on its application domain. 


Much critique has been applied to methods aiming at interpreting complex and potentially non-interpretable models. 
Some researchers argue it is not worthwhile to study non interpretable systems while dismissing that using inherently interpretable models in the first place might be the better approach. 

Adversarial attacks show that machine learning systems are still fundamentally fragile: They may be successful in a number of tasks, but fail to adapt to ood scenarios, i.e. when being applied to unfamiliar territory. 
% Our results raise concerns on how interpretations of neural networks can be manipulated.
% fail unpredictably

 The findings about manipulating interpretations do not suggest that interpretations are completely meaningless, just as adversarial attacks on predictions models do not imply that machine learning models are useless. However, they suggest that there still are fundamental flaws in the way neural networks operate und that much caution and supervision sould be applied if they are to be deployed in the real wolrd .


do  notsuggest that interpretations are meaningless, just as adver-sarial  attacks  on  predictions  do  not  imply  that  neural  net-works are useless. 

This paper follows the footseps of \cite{lipton2018mythos}, trying to caution against blindly putting faith into post-hoc explanation methods. 
Moreover, we propose that checking the robustness of interpretation methods not only with respect to adversarial input manipulations but also with respect to adversarial model manipulation should be an proof of concept. 


% We argue checking the robustness of interpretation methods with respect to our adversarialmodel manipulation should be an indispensable criterion for the interpreters in addition to the sanitychecks proposed in [27];


%%%%%%%%%%%%% FUTURE WORK
\subsection{Future Work}
We see several possible future directions of future work. 
Firstly, for approaching the discrepency of in-lab and real-life applications of, more focus might be laid in the development better performance metrics for both measuring the performance of machine learning models as well as their interpreters. 
More specifically, it might be fruitful to further investigate the correlation between ood samples and the performance of an interpretation method.


There is also no work proposing a benchmarking for ... 