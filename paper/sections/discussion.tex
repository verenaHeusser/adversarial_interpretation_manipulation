\section{Conclusion}
\label{sec:discussion}

This paper summarizes the current approaches to manipulating model interpretation methods. 
The main insights from literature outlined in \autoref{sec:manipulations} are the following: 
\begin{itemize}
    \item Saliency-map based interpreters can be tricked even by simple perturbation methods, such is input patches, which also succeed in fooling models. \cite{subramanya2019fooling}
    \item State-of-the-art interpreters may not be able to detect biases in biased and adversarial models.\cite{dimanov2020you, advlime_aies20}
    \item Biases can be encoded into the model by adapting the loss function and by inexpensively fine-tuning the model. These adaptations can trick the interpretation methods into yielding wrong results while models remain accurate. \cite{fooling_nn_interpreters}
    \item \cite{dombrowski2019explanations} show the pendant of adversarial model  attacks for interpretation methods: They apply visually imperceptible perturbations to input images, that do not cause the models to misclassify but that cause the interpreter to yield significantly different interpretations.
\end{itemize}
On the one hand, the findings suggest that our models are not fully aligned with how human information processing works. If machine learning interpretation models would decide by the criteria we humans employ for tasks such as image classification, there would be no fooling of interpretation models by input or model manipulations. 
On the other hand, it was shown that advances in machine learning models has led to models that rely too much on the data they are trained on (the i.i.d. assumption), thus showing a high susceptibility to o.o.d. properties or properties that are highly correlated with labels in the dataset but are not distinctive in the real world (such as image backgrounds) in the first place. Models and interpreters can still be misled in a large and systematic manner. 

A growing number of studies gives evidence for model how interpretation methods can be gamed. Among these are the studies outlined in \autoref{sec:manipulations}. Other studies also raise concerns about if standard deep learning practices are valid, such as the work on fooling the broadly used attention mechanism \cite{jain2019attention}.
However, findings about manipulating interpretations do not suggest that interpretations are completely meaningless, just as adversarial attacks on predictions models do not imply that machine learning models are useless. Nonetheless, they suggest that there still are fundamental flaws in the way neural networks operate und that much caution and supervision should be applied when deploying them in the real world. 
This paper follows the footsteps of \cite{lipton2018mythos}, trying to caution against blindly putting faith into post-hoc explanation methods. Moreover, we propose that checking the robustness of interpretation methods not only with respect to adversarial input manipulations but also with respect to adversarial model manipulation should be a necessary proof of concept. 

While there exists a number of review papers on XAI and it's various subfields, this report is to the best of our knowledge the first one to comprehensively review manipulation methods for interpreters. 
We believe that identifying risks and adversaries helps to open up research on more robust interpretation methods. 


%%%%%%%%%%%%% FUTURE WORK
\mypar{Future Work.} \newline
We see several possible future directions of future work. Firstly, for approaching the discrepancy of in-lab and real-life applications of machine learning, more focus ought to be laid on the development of better performance metrics for both measuring the performance of machine learning models as well as their interpreters. 
More specifically, it might be fruitful to further investigate the correlation between o.o.d. samples and the performance of an interpretation method. So far, most of these findings are limited to specific experimental settings (e.g. most research on interpretability is focused on computer vision tasks).
Further research should much more explore real world datasets and tasks. The relationship between different interpretation techniques and the dependence of interpretation susceptibility on model class, interpretation method, and task type and dataset structure should also be thoroughly investigated. 
