\section{Introduction}
\label{sec:introduction}

In recent years deep learning models have demonstrated superior performance in a number of tasks. While the performance is still rising and more domains of tasks are accomplished, these models still remain black boxes often uninterpretable even by experts. 
In many domains, neural networks currently are the state-of-the-art solution. However, as their superior performance comes at the cost of complexity and thus interpretability: The models often employ millions to even billions of parameters in order to achieve universal function approximation.
% https://dl.acm.org/doi/pdf/10.1145/3387514.3405859?casa_token=lCc16GOTZsEAAAAA:gypLNU1o2Wwl3wt_b8stRbb0mgxEomX8PWprPeciNdkhVften3-5E01RM50e0W9NGQaGd4TrLOhA

Thus, automated interpretation methods are required to make sense of the reasoning process of such deep learning based models. 

This issue prevents this vastly advancing technology to be used in high-stakes and safety critical applications and prevent real-life deployment of such systems. 

This article reviews the current state of the art research in the field of model explanations and model manipulations. 



While most of the approaches to explainability focus on the application to computer vision tasks, other areas are seldomly chosen. 
More importantly, while a big motivation for the development of robust and explainable systems is to overcome biases in models, datasets with 
direct implication of biases are seldomly used and by far not treated as benchmarking scenarios for explainability analyses.  



% Outline
This paper is structured as follows... 

I