\section{Sanity Checks for Interpreters}
\label{sec:sanity_checks_for_interpreters}

Checking the robustness, scope and hence the quality of model interpreters has become an indispensable step in explainable machine learning. 

\cite{adebayo2018sanity} propose a number of randomization tests for saliency-map based interpreters.
The authors find that most methods fail their tests, and warn of the danger of visual assessment. 
 



Additionally, in order to account for adversarial model manipulations, Heo et al. \cite{fooling_nn_interpreters} propose to expand the criteria for checking the robustness of interpreters further.