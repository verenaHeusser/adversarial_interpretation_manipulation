\section{Uncovering Manipulations}
\label{sec:uncovering_manipulations}
An interesting question is, if adversarial attacks can be uncovered and if attacks can be removed. 

Adversarial attacks on models using input perturbations can be detected to en extent by adding small Gaussian noise perurbation to the input. 
