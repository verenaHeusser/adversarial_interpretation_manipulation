---
title: "Why you shouldn't trust me: A survey on Adversarial Model Interpretation Manipulations"
author: "Verena Heusser"
institute: "KIT,  Intelligent System Security Research Group, Seminar Explainable Machine Learning"
topic: "Pandoc how-to"
# theme: "Frankfurt"
# colortheme: "beaver"
fonttheme: "professionalfonts"
# mainfont: "Hack Nerd Font"
fontsize: 10pt
# urlcolor: red
linkstyle: bold
aspectratio: 169
date: \today
lang: en-US
section-titles: true
toc: false
<!-- today i will present a survey in the field of explainable ml: On how to fool interpretation methods -->
---


# Omnipresent Machine Learning

* Verification of machine learning (ML) algorithms mostly w.r.t. accuracy and efficiency 
* Also: 
    * Regulations: 
        * Right to explanation (GDPR 2018)
        * California-25: Cash bail 



 
# Motivation

# Interpretation Methods

# Adversarial Setting

## Adversarials: How to fool a model
* Adversarial examples \cite{}

# Interpreter Manipulation Methods

# Fooling Examples

# References

# Sources


